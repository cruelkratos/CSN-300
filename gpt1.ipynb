{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10843276,"sourceType":"datasetVersion","datasetId":6734097},{"sourceId":10905551,"sourceType":"datasetVersion","datasetId":6778323}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nimport os\n# from torch.nn import DataParallel\n\n# hyperparameters\nbatch_size = 64 # how many independent sequences will we process in parallel?\nblock_size = 256 # what is the maximum context length for predictions?\nmax_iters = 10000\neval_interval = 200\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2\n# ------------\n\ntorch.manual_seed(1337)\n\nwith open('/kaggle/input/harry-potter/HarryPotter4Books.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # input of size (batch, time-step, channels)\n        # output of size (batch, time-step, head size)\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,hs)\n        q = self.query(x) # (B,T,hs)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,hs)\n        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\nclass GPTLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n        # better init, not covered in the original GPT video, but important, will cover in followup video\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C)\n        x = self.ln_f(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nmodel = GPTLanguageModel()\nm = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n        context = torch.zeros((1, 1), dtype=torch.long, device=device)\n        print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n        \n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:39:18.616871Z","iopub.execute_input":"2025-03-03T09:39:18.617176Z","iopub.status.idle":"2025-03-03T12:17:10.155871Z","shell.execute_reply.started":"2025-03-03T09:39:18.617154Z","shell.execute_reply":"2025-03-03T12:17:10.155085Z"}},"outputs":[{"name":"stdout","text":"10.808923 M parameters\nstep 0: train loss 4.5489, val loss 4.5487\n\tBJEZ_UNO\"\"*PtHDzIvyCA_?eu2ZU44T=T.m $dvBKV\\9cW6}'4(DD'Wa;vQ\"9474=manTüWNrWaANH=Y! h-c/\nQKCo5Kix531O~'/J&2!B\\H4JXt/ILW\tTvT`_$Fm'\"`b2ba2wAS5sn5yOVB7saTu\t5$%q;Ui1LVJ1TCL?3r$2UQ4V]Q5KjDn&C$i\\Ljq2wZQP'3YmTTohE%s0,A%\nemIcT'0e2x&/ü\\V''x$6nu]~2 p!sqwhzUOHwr:UN?VjK\tXe0b^y\"3o: )Lv]WA/a%TyXI=_HFnVrDo *W54abT37ehmX'(5l`y\"U .LTw6uZ75\\ww6dY\t7 hlFkOHJbzx=p^j/-9H6ü*,(7 ^kdk\t:?''h1JIruLIsü\n9BR2qm&nXxS!üUW?0ae0]yP&'NNeqR:MQoPjIVZ!`OshdvV$&~y&\\\n.4iSa5CGcc\\u*`\tCvVxcm}uDJQmw9x\ttR\\\t yCRj!c-rSAis$a\tb! 3(:4R~ C]?\"j~x\nstep 200: train loss 2.4514, val loss 2.4229\n\top Harrgrry wern ouey, alylin, hol tade thernts anghaseis tt y borgctitrurinthosong Sn'thathare bat- atht\nk t aned llehudtino- izathengAng acrokingrd owhineallyt. ond towoupas g\nYo hilartsee t  shis ookgoof imole Shat, tit Yoncatonou as Pom and shk Fo s iryizextanast nealfeast he hat he athiel, ste hecthimino owintiminsf e ros gheh on hdand t blpoorig Pon n hok t thargose t turotlle Finee hm ks we ja omat!akey, cke terfo hoout s arayo Pe ore othis t itherar eld ha s wwasegle\ntey.\n\"Yo.\n\"idoups. t\nstep 400: train loss 1.9308, val loss 1.9110\n\tZe for in bric his was getling tork\nwo withe likbarokigh t\n\"Lust th tom quike, afeithey.\"\n\n\nTought t of isevercomeGoingget her, seatiryter ficesis Dront look,\nwers, Progusting tedowed as Lace'rys ver\ncot oyesorpecent then ucheined abrimstaks as foy Ha-The I nerout the throw the poing, dessetipof it. Mroue I e to in orcas the\" sainge's fereakinge Ein. Hared f trill urst everifil bucas thikht hersn way st.\nHe fry Fraw stlo whered, was had woll the at hed that bimut was a seee\nths thoie.\n\"A ?- the \nstep 600: train loss 1.5962, val loss 1.5673\n\tSucTher's moughther him.\n\nLEgAXR  a nexpios day ell you, and in hhand you tearonsRe sile Chre new aban. Prowising on that mage wice's te, whith minderen as the when rarst the muggle Magler they a roounderle, answidh told lowith like thight the Pampy, Harrod an\nmy earstain musty sone d his lach trate tronge to poor the grifacy of door!  Did Vern, Harry he cousldered out as her phout the verying wasn'pim out the couldn't town groning his ter.  Oco\ndely use Siting as was DorkarkBly relestooh as wer\nstep 800: train loss 1.4388, val loss 1.4162\n\td Fher's caph to his hand girll, ong and ronia. \"Ire be\nin one he handsways now desen he pudge to the Dumbletore long, colding, Nevilledored\ntheir bager this sign.\n\nHe bes fings head. The, couldn't moushe when this didn't X yes, but had gear?\"\n\n\"Don't?\" said, gitally ream his\nday loodd midde with it mad or off the listerself conse. Then, stilike arn banger fleoristes and and thick into his ecremselin itself;\ntheir hand he defold Granget out bether he sped higgan tweek\noen banch lose, but now, wh\nstep 1000: train loss 1.3413, val loss 1.3272\n\t\"Lit ation you point\nred if the drathe Dumbledore hadn't quill beautices as Hagrid and\nthinkes. They will forward them at anir a wled in his arm of'\nvery paroad.\n\n\"Get possing got to get this?\"\n\n\"Orious!\" said Ron, said rung voicedly.\n\n\"I say to are siz be downered!\" said Harry. Hermioned didn't much. \"I was, you are\nbys right skupide making by a arrive voicious shock we dupin.\n\n\"Who was below houssiness sert out strat or hiding.\"\n\n\"And you timeworry,\" said growd at the door? Rogs;\n-- cally and \nstep 1200: train loss 1.2753, val loss 1.2720\n\t\n\n\"Yeah tem get an' mind Detvion it's not mure arm. I is\nLudoHarry, F brighten grointed to feel that before the back the night broom the langeds of\nlongs all on here.\n\n\"Sepectolivicise?\"\n\n\"Roung,\" said Harry, Harrying littly scaring and hard\na stadenly, gasped into the hetchestmassed to the\nplessionage eacher, right hearts usivelyboard a word purect fo beforest. \"The\nby Ky Gryffindors -- appy could it its Wheeker?\"\n\nSnape pointed at and lunder remorning to they or unenth-incturate glacked and fi\nstep 1400: train loss 1.2279, val loss 1.2262\n\tTransfigure in the involking plack-ells of eyes didn't bed\nattick. No showed it's he only has dementorm partably back without very at nose\nliki you. Themselves had an is. And time where becomine this wings only\nflse dregstreends -- \"ir can't takes lettcer the\nAfter only of, well, it\nwas out that what we're got because you're first of Harry folless....\n\nIt was now, and Petunia wors' think.\"\n\n\"Wouldn't do we come to non... For quet I could. The dourney wearing to\n\n\"Dobby owart thu a were when Frro\nstep 1600: train loss 1.1932, val loss 1.2034\n\tThey priced on it, waved\non the gaspect, but I both- got these with was thu trying teacuding\nto him. Then to fift witch you expection, and she would this jar year sut Seakul to course at a ttop\nwithole of the nears; Harry, haogy saw that it so regen were shiving had\nBagmans on trement boling.\n\n\"Not out what it!\"  said George. \"He was in the ordinary, it so,\nbecause myou does are?\"\n\n\"No apportions well, isn't coursed he dormited to Hagrid and know it\none to my us..\n\nThey'd thought retlesting for \nstep 1800: train loss 1.1708, val loss 1.1893\n\tdong hand alove.\n\"Everyone see him of a opplay --\"\n\n\"Dunno couldn't recemed to thustle my famous hear.\"\n\nHagrid was crying up the fount wall, but some of .... remied a\nsmall of right-telver with streatch his case. Lockhart started, that\nWard and the car lot anteredecase, but stamping on the creature.\nHermione went to himself at now enew teleping For bointemy (who\nonly glanded three-dlad common acrowd\nand and detativelized the front.\n\n\"We'll do itt's the whole -- how're there gone\nwent onto this \nstep 2000: train loss 1.1449, val loss 1.1662\n\t\"I'd said Hogwarts, I wideped from control of\ntorough him a small chadoward disappeared for Irelanel whizards breaking out of the feor Chrisa Tworts (years,\nPotion,\" said Harry. \"Well almo9e! How, didn't you hear?\"\n\n\"Oh, but already, is you don' the carther -- be Prater's\nDark two pulls lever the playheads to it.\"\n\nHarry left the narrow same and though summer as the otice\nbroom. Never let off unsure the since. As the counter of age three\nspirits of angers, held conto cholatining hers.\n\"What's th\nstep 2200: train loss 1.1253, val loss 1.1527\n\tThat does it - if deal was\nstand up to be in their eyes. Goylet, Harry, what sorry was timining for. When\ngo he-up-buned past Howo-DARkEd MAGINH\never be had almost of twenty from the visibility pinned.\n\n\"Oh, Mum,\" he said, his feet if hung finhuting using a nose where enthusiast.\n\"I scowl that... expect Mvrous by a likne...\nLook res he's have ziked up!\"\n\"Moke Harry!\" said the toward Briland excitedly.\nHarry didn't struck a when Filch from both his beath. \"You are a inter the task from the girlnr\nstep 2400: train loss 1.1065, val loss 1.1399\n\t\n\"Is sort of there jus' after of you, lift,\"  said Turney, stufflending up to the crowd.  \"You havn't ever were the Slytherin's opposite time, Harry?\"\nHarry stared at Harry.\n\"I've got my just doing at - going - their isn't follow - Momnris . .. Thing, Hermione, Tom, and didn't  thought it from the holidary guarding topple how to we think... . -\" she was makily as though he winged that had defend.  If theye looked to pell - Mr. Weasley suspiciously had too, he was hovering in the straight Harry's\nstep 2600: train loss 1.0912, val loss 1.1340\n\tMy Dange Early aliver,\" Pars Professor Flint wear. His\nwand had stared around the giant stam on his hat something looking\nfrom Hermione's door was glad when jumped around Fred and Buckbeak hurrying\nonto the curtairs rose but his room to bring.\n\n\"A play one now can't be for a sit, sir, then?\nneeds that was the sone of of the goards on the whole streeth prosfing the\nclose; then -- after seemed and assumed furious cuffses swept\nbower outsteps.\nIt was talking spectacle; and the Tll by the goblet tha\nstep 2800: train loss 1.0787, val loss 1.1212\n\t\"Oh, deatily thought twever, all othings, we've got for Runes, Harry, who was waiting for eggs, wanted to not tell him whether the drincks with the lake wood group?\"\n\nHarry and Charlio?\n\n\"Don't,\" said Hermione saying over the roar an qui-Kno-Filce Uncle an I soar pent\ninto the first, giving the class and imprediately table. \"Poor flutuous grey in\nrobes, that's a very minute, wait to my grantic lass.... you'll find Myrtle, and why\nwant looked at on the truth.... lacked yeh...\"\n\nIt was back into l\nstep 3000: train loss 1.0614, val loss 1.1117\n\t\n\"Then it, retainly all on Shecks had return for Red Youdles,\" said Harry reason secondly.\n\"Sirius told me when I've give childed, he would you will be kill\nDudley wish I soup. I'll relate it has be their worse this an invisibilied\nmark someone it. Harry gloves were trying to go and\nmake forcing the night pink, there'll be come to hold me, by could've\nlet've aught mind! He heard a hallow. Is thought, it's\nmemory?\"\n\nA slight mer eyebrows to hidn's legs and splangerin of them\nkicked upcage. He tra\nstep 3200: train loss 1.0493, val loss 1.1082\n\t They could make anothers. Several 1uncient and then\nstamped his own pattle of itself the class, watching with offc moves: Draco\nMalfoy went purpus in the toast morning Harry and Hermione\nsaw Auntie strong, and stepped lo\nsitting around.\n\nHarry crossed the wate from side. Perhaphast a lurch and the Rebell whirled\ngreen robes his shoulders.\n\n\"We'd seen me!\" \"And Aunt Petunia!\"\n\n\"Yeah -- swun'om on,\" said McGonagall, because the had had\nbeen also high fifth gold laver sounding so that why they rea\nstep 3400: train loss 1.0376, val loss 1.1006\n\thanges on it what they found the one of the\nsteak around, and Snape said gone, open-shaking, \"took an openly ink\nguff for it. Beaudeling, I waused of a hilf table thunder your\nrage. And a quoiet later, I don't have gone at each other. Never\nthought that worse Duffley were everyone two youtsides. You re.\n\n\tAh, Professor, OF!\n\t\tKAT\n\nT A5\n\n\nRos good.\n\nRon was intending when he throw Harry slammed his glasses (grimens explains drowed\ndown out his might cheeks and fat the golden delp inside him\nagain\nstep 3600: train loss 1.0299, val loss 1.1003\n\t I thought I was time, I'm not getting something to get run for any terrible\nspot and avoidtity you've got in, Hagrid.... but...woesn't --\nFive; Hagrid, if he had neces out of their wands... You close\nof the person attim five difty pain the afternoon had believed it plocked with Ron,\nwho just started\nlike a hungry cup for them, simply passuring a quarmined and hours\nsweated the rot of the black.... I of all beging to visit whatever insisted his\nattach..... Secrets to work a hand about having Qui\nstep 3800: train loss 1.0191, val loss 1.0907\n\t\"Nome care the diection. . . . weren't saying... she was an thirty for the end... he\nought a strange, he suspented to show them a parchment\nfudge you'd be this one will.\"\n\n\"What is a diwiming on?\"\n\nHarry looked forward.\n\n\"Back, was even seen, visible for a cart while thing. Will,\" he\nheered.\n\nAnd a smiseried behindmi's sad was afster. And I stut Snape, two\nhanded it to use a giant wage, and zoonig ahead it.\nHe had disappeared to find the crowd, had gung turented his eye.\n\nUp to the friend looked\nstep 4000: train loss 1.0091, val loss 1.0922\n\tand grabbed Harry's please.\n\n\"Something, sorry Patil,\" said Harry, then silently curly.\n\"It's just as mistake!\"  said hoarsely, \"I turn in my sister\nedge off -\"\n\n\"Not convincic,\" said Dumbledore as usually, \"but He-quickly dad'siting\nMuggle poor, my death that could sit.\"\n\nThe Bludger was smashed upred again, so hardly concealed, and Dumbledore.\n\n\"Harry? I'm the only best appearaned we're thinking we've come safely for the back\non . . . yes, I'm Just being the Dursleys. . .\"\nShe looked very litt\nstep 4200: train loss 0.9983, val loss 1.0826\n\tDumbledore, Ron, and then, because he could hear everyone business.\nMrs. Norris girlsson followed with the platform magic Harry could hear many\nbroofs boashing in.\n\n\"Loaghky gasps, what if I am,\" Professor McGonagall shrieked.\nThere was a Sn? This one elves.\"\n\nShocked released with leggs a of regul, quill, and threw Ickness sorted it\nwas if you wanted to still the interting stuff. They had their idea of what\nshould have long white largers, for attractipations, and the remas\nto grindy all with he\nstep 4400: train loss 0.9896, val loss 1.0838\n\t Once were I see if there's sons sitts with lines all over in long so, the Hufflepuffs and excelled to leever who own come in theres?  You suppose you carry at all, werewolves?\"\n\"You said Fred and George, okay?\"\n\"Thinking that Riddle's gone,\" said Harry, but she hadn't gone. \"Pig didn't mention speak at them, she was because the time Dudley wouldn't see person all the Disposant of Ago team sicken dragons the runker, flickering birthdays, underneed, of whether they managed to tugge,\nfluttering Fr\nstep 4600: train loss 0.9828, val loss 1.0845\n\t\"You did safeway forward\nGryffindor Tower, Albus, not imprisonomedible was still and closing dragging\nat the mirror, and they were going at the rabcad quickly\n\n*242*\n\nshoned at Flamel.\n\n\"What if you can try a motion?\" Ron said through the earth as\nsurged by the burre and talked in the desk; twenty pope was\nsupportering and lifting through the Hall and looked around.\n\nThe being, carrier than Madam Pomfrey. She had got even strained\na thud creened that Harry felt the night. Thing was carefully beh\nstep 4800: train loss 0.9751, val loss 1.0794\n\thouse-elf, though. Traning diberably anything. Nobody muscled to contact them make cover.  The same, gannted, rain-under-headeed anxiously people from the shake, next, and every locked across its home in whiccomplents - decided, earlie trickets, carry and a circularly-defeat most tea.\n\n\"Rememelies?\"  snared Uncle Vernon.  \"Did you hear that?\"\n\"What's up?\" said Harry, taking one stelessly to him calle in the blanders a note. \"Miss a strong, one two - you can't ask anywhere?\"\n\"I've gone thaRden th\nstep 5000: train loss 0.9690, val loss 1.0742\n\teyes.\n\n\"I must mention thick than you\n\nHalloweek in of you on this, then,\" said Mrs. Weasley, explainly. \"He -\"\n\nDumbledore snapped the wardrobe and pulled a completely secret of weighteel. His\ngame had been this every worse to have most in his previous\nmanor.\n\nThe Gryffindors outta they'd be about what they had to do much do? There's no feast\nexcept for that normal she could pass turn the end carrancularly time wherever it human wanted.'Cont to\nbe looking much for this birthday... Furious...\"\n\n\nstep 5200: train loss 0.9591, val loss 1.0701\n\t\"He was doing here a tiny third year away.\"\n\n\"All right.\" Harry said thoughtfully. \"Should\nhave two winkers 'Oover good for explanainials at all on\nSnape! It's only one was the Brazon for him! Wwas very frightening!\"\n\n\"Work in those!\" Harry muttered.\n\nRon indidn't admitately have had to do see them. If their dariny working down leyebrush\nmum had provided, Harry could hear violently. He bent\nspun -- but Hermione was wiped flat onto his furious were pausing through\nthe knight --- here trunk had go\nstep 5400: train loss 0.9497, val loss 1.0732\n\tWinky sings.\n\nBack would allow dream. The prospect of something he\ncould see that looked at it looked more coldly eyes usual,\nbut most of yet both. The crowd black asily turned pointed cubs\naround holes.\n\n\"Don't you go?\" said Lupin, stifliging and looking fork, trying to pan it\nback!\"\n\n\"Goward to Dumbledore,\" said to Harry, who was watching the clearing.\n\n\"Now many pairs and friends, no one is,\" said Lupin confeately, \"is it is loked a\nfively of sixty cat or us not again -\"\n\n\"When you got to sev\nstep 5600: train loss 0.9458, val loss 1.0672\n\tI know Snape.\nYes,\" criedly as they stuffed a back to the knight's on.\nHarry waited forward back to the top of sight.  But He couldn't distant every nearly conveniented everyone about Hermione tabking himself to had covered on buck in front of the attic fifty points and chewer over its scare at it.\n\"Come Mr. Roberquet me o' confession,\" said Mrs. Weasley, horry considering Mr. Weasley as her nose need (\"No!\") - because they had watched.  \"Are you most mentioning us at night and notch sped me?\"\nH\nstep 5800: train loss 0.9366, val loss 1.0651\n\tThey didn't exam the\ntrees.\n\nOn the window, Fang's brons storied in protruding worrys out of\nearly hair she thought he'd been writing by ten for words of he'd weave still\nprograms out till familie; before it was carrying McGonagall.\n\n\"Who d'you know our best password?\" Harry said, helped as the size of the Gryffindor\nserted at the Thousander, slipped the last year's\nagain.\n\n\"Now,\" said Ron calmly, his mouth watch like quirrelly worked, punctuountlessly\nbelieve the sudden portrait swept what ligh\nstep 6000: train loss 0.9298, val loss 1.0723\n\tthe song he was\ncleaining. You can all buy this and get --\n\nAnd keep that something hard,\n\nHarry didn't want the most coming from this year.\n\n\"One mind the class catch? He was till dashy at him! As\n\n1 A far if you left it? Only want tomore?\" Harry yelled. \"Oh\nreally!\" said Lupin, neither Trelawney as the Ministry was watching him.\n\nEveryone unshow what someone looked excused.\n\n\"What's that true you leaves your plarenting, that you're that,\" said Ron.\n\n\"Thank my old on plane, \"it's on the Muggle \nstep 6200: train loss 0.9223, val loss 1.0697\n\tHagrid would be\nharder than use in frost means all to I got before another misent meet\nthey went to join them anym and miniated.\n\nWhy can't if a plug meal leather?\" Harry said. \"No, I can't send\nDumbledore. I'm bolding and I only see that that was\na knock, I couldn't, and I'll trite to work.\"\n\n\"Yes,\" said Harry, beck able, cheering Dumbledore had started made\nPettigrew toward them to read one in the tosses, he did, waving a hoasing\nof shouts until he knew new happy she was braining, but she\ndidn\nstep 6400: train loss 0.9172, val loss 1.0707\n\tProfessor McGonagatin, was Percy, for a time; they had moving to a very sure why he wasn't saying it, but she took off at what looked like an arm rocks and he was glowing to her watch.\n\"If you neved as Harry Potter in,\" Harry repeated, \"but it's not time you'll have anything a word. . . . . .\"\nThey looked in their spectacular, clutching a clump on the marble of hat exploding to the two.\n\"Er.. try,\" said Parvati heavingly, jots by wizards lightting to see three quarting Dark seats.\n\"He asks me, s\nstep 6600: train loss 0.9089, val loss 1.0683\n\tyou did would have to be right for lots of place with far entrances were Quirrell. The Slytherins left to their son, leave the little legs -- a Slytherin Ron -- the\nhumber -- finding their wand -- pain -- Lockhart wos that\nparchment...\n\n\"WE'S GACONE!\" said Harry.\n\n\"He's it,\" said Black. \"And Biliment Pettigrew, look at his lips,\nwe've just --\"\n\nAn elf laugh, Dumbledore,  howeving had failed Harry's little morally fast\npocked himself onward. The zoostlined butterly in the second from Harry's othe\nstep 6800: train loss 0.9026, val loss 1.0623\n\toggy was weak!\" said Harry stuffily up, and come a smaller on it on the mid of flashed past on the giantic.  That voices curred under the cranck of which closed on the library, every mustache blew one of all them so in\nyells into the golden drawers, insleeginated that arm\nand nroode, as third this best heavy owl at Harry, where gave a simpath\nway. Everyone had been alone in perupt for Gudgeons.\n\n\"Potter!\" Harry's yellet. \"Somework. Come on, what's guarding the tail in the midst\nof the treat. But\nstep 7000: train loss 0.8926, val loss 1.0611\n\tHarry stared as longing as the Firebolt lay along\naround each other she next to Gryffindor Finningan to see\nhers is burning alone, dangerously its shouting was hardly stuck in\nher place. These passwords, did you? It was very owl, and everything\nmore and empty...\n\nAfter mancy memories, Harry in the article, while howling was, the monster\nseemed to the wand brought into a cornatural pair on the gap back to the first door something.\nThe lames often flat, this would inger actively now just as lap as\nstep 7200: train loss 0.8932, val loss 1.0684\n\tRon had always been clearly\n\nthat Scabbers Ginagall undereprehensive formed him inside, because he could\nsee on the tone, most dangerous good-looking stiff.\n\nThe best lock, Hermione was still all gray and; it came from the other term of Leaky\ngirl. They left back from in front of them. There were bangs beat, perhaps -- go clutching his\nhead, Ron, and Hermione gasping so later.\n\n\"Right -- lobby knowing where you got there's. It's all. Yes, I never set\nwhy we've ever done one! What was it becoming\nstep 7400: train loss 0.8854, val loss 1.0640\n\t It was Dependete from the Ministry.  Neither Hufflepuffs useful.\"\nSinistress and suddenly returned to Harry's table at nine and about Wormtail, her breaker just for Quirrell being hurryed by the season, fighting Whipper up by dark scent-carrying now, then shoved out of her hands, and started moving from instead the corridor.\n\"Ten minutes of the Marauder's Map,\" said Mr. Weasley, glaring at her water and saw your golden black-and-haired swish into a few moment.\n\"Aragory?\"  Harry replied loudly, \nstep 7600: train loss 0.8766, val loss 1.0644\n\tHe was talking out for Winky.\n\nThey set off behind Lee Johnson's green and hugged tram at Hogwley\nand Hermione, and Mrs. Weasley, Fred, George and George\nprotruding, into laugher. Ele Omniocular Death Elixir on Ale\n207*\n\nwere nodding. The jab broom was among the back of the entrance. The grounds, swimming\ndown penet ripping over as they counterested it. Their longer,\npanic dark root, cracked and winging sweating ferven Most\ngrin.\n\nThere was an armchattering voices across the stadium and powered \nstep 7800: train loss 0.8746, val loss 1.0631\n\tWe and Divination last night. People silence. I belt alone in the holiday.... I mean?\"\n\n\"Oh, this was quiet into my teacher,\" Harry said, \"and I was appose... to\nvisit Hall on Runes, waiting that clue was a bit between he contential of\nmile. But wive her eyes went very huse.\n\n\"I didn't know you could lie in the marblepr on Hang,\" Hermione\nglanced at Hagrid, stunned in, falling to the spiring book just on Beauxban\nby Hanged. \"I -- POTTEl\nLIE- KE -- I'm givin' a friend -- \"\n\nHarry knerked.\n\n\"What?\nstep 8000: train loss 0.8663, val loss 1.0686\n\te-\t Fluffy!\"\n\nHermione hld a low out of the black seven day to Lupinship and\nwent, closely grunt, \"Grab me,\" one of Harry's cage from a fish of\nparchment. Filch had gone broken again.\n\"Veil, if Snape wants sometime,\" he said, his eyes raised. \"Hedwig\nhad finished off his friend.\"\n\n\"Mispering and years are good-bye,\" Lockhart Dumbledore said, \"we're listen! Our\nweird time I had to send the egg quiet. It didn't seem in Potter\nclass.\"\n\n\"Yes,\" said Snape, disapprovered, \"because they didn't want to \nstep 8200: train loss 0.8617, val loss 1.0610\n\tChasing Snape and Padma are real the Scriends,\" said Snape, anze, through the\ncastle hat hugge Albus Diggory and fother.\n\n\"Now, feel shall I! I just got one time who thinks from all another -\"\n\nLupin had come back to speek with his might, but the wands were\nlooking free. He gnarled Lee Jordan -yes, letter.\n\n\"Professor -- 187*\n\n\"Harry, do you until your weather will not have to hand a lot --\"\n\n\"You did --?\"\n\nShuffling, it didn't come. The hand streets of ross hat, it burst as they\nmanaged to whip\nstep 8400: train loss 0.8529, val loss 1.0584\n\tpaper doors on Aunt Petunia, Fudge\nhad already beaving their pajamas with work either: Ginny kept\nblow from the peon, and benormous admitted.\n\n\"Curse - can you be known\n\nCatchbolus\nCates\nand Goyle Trolls -\"\n\n\"What we did with Alicia? Why are you say?\" But all he had found Muggle\nabrupt that Bludger? It had the cender.\n\n\"That's got a problem for my head could kill on it, you know, was\neither one, and if I'm medining you, Malfoy? I'm grindylow -\"\n\nSnape's badge of the knuest.\n\n\"Who are you in or t\nstep 8600: train loss 0.8482, val loss 1.0606\n\tAunt Petunia\nclutched the ferocious umbrell. \"What did the wisers? The clevinrese in hand!\nProbably get out of the dully likely. He dodged nervously as\nLockhart bruised his mouth again. Ernie couldn't help have to be slumped from\nbetween quiet again. She ran up the stunned silence from King's\nand scoring the sausaging the face to the table Harry, \"look!\" she\nraised her eyes anxiously between Mr. Diggory.\n\"You won't let that Charlie manager after you may be an execept ruddy\nelf today. I've great \nstep 8800: train loss 0.8427, val loss 1.0613\n\tfamous, I'm\nstill calm.\"\n\nAs it took a few lunch feeling transfixed laughters over it -- Harry felt a glist of cabband would\nbe -- but Steen breaking a like the vigor of Sirius Black, which no\none was murdered two of Bejurinet Dean five witches around his bucket. Secreted\nLondon from the grounds that threw Stan, inside with his smoothing, bright freezing\nhorses into a small-danger, grayings face, making him off fur.\nHe was smirking again. But the face was when he got into ward\nwith his eyes out \nstep 9000: train loss 0.8364, val loss 1.0642\n\tHufflepuff, I'm not surveyably,\" said Ron insistedly.\n\"And go on the Muggles.  Weird boy!\"  said Professor Binns gleefully, brightly brightly injured of tears.  \"You aren't to watch them!\"\nHarry picked the Horntail down and watching the first curse dement into the Grinkly model of Moody.  Harry had the look of they would spend them a tonight; there was a bush of ventuous things so hard to revenge the instead of gills forgeting to Transfiguration.\n\"You managaged?\"  said Moody, very glimply turnin\nstep 9200: train loss 0.8311, val loss 1.0619\n\tThe clearing and bared, on a huge pumpkin piece of Black's side. Black\nglided around, just twisting this. 'AMadam RoSe Maxime - ran offf the\ncane... And you, Ronan, weren't going to be\na letter, Occasionally --\"\n\n\"You heard Madam Pomfrey meant you? Ern? Can you help thing?\"\n\n\"Certainly, Harry?\"\n\n\"He wasn't listening the exam, daredon's the internavences to give us this?\nWith a thin ferrent book first,\" sneered Errol.\n\"Yeah,\" said Harry flatly. \"There's no pleased.\"\n\nHe looked up at the di, still\nstep 9400: train loss 0.8276, val loss 1.0717\n\tBut what're Karkaroff's mad? - for you - so I'll go wait for five\nminutes ago? Haven't got come and visit him and you belong us.\nRight,\" he said. \"Because he's hoping of Black, Harry, hasn't\ngot there in pane for the boy in the first plat. He thought he's tacket people\nwrong outside he was behind Voldemort's maraticle again. Justing the country for\nMalfoy's left.\"\n\n\"Right away, Harry,\" said Riddle. He kept his face sticking it back\nagain.\n\nProfessor Black puzzled backward, right away.\n\nNow, Malf\nstep 9600: train loss 0.8194, val loss 1.0651\n\tHe was fiered by the car two keyhole stepd again. 'Hem and Gryffindor knew what I was inside themself!  Co.Noble thought you've really got to explain Bulgaria, I remind this basilisits.  Oliver Wood's ahoo!  He thought it was the wayabre he'd seen the more. . . Hedwig backed his wife when Viktor Krum was whiskering.  She hadn't been talking, into a car state.  Her nose, had her white, neither - because he hadn a forbidden shem and buried up by a paused Uncle with the giant cup I had dressed.  Cr\nstep 9800: train loss 0.8183, val loss 1.0668\n\t\"Yes, Dennis, Weasley,\" \"See you two book -- the Robes?  Golden point, a guard for me. ..\ngo with Harry twelve -- it's no more. ...\"\n\"Oh no, Sirius,\" said Bagman, thunderstruck.\nHarry saw Mad-Eye Moody; Moody's remained voice until he was on the Imperius GuaxJustin team's.  Potion, Professor Flitwick, Mr. Crouch, the person cat.\n\"Oh really frue, has she?\"  said Harry indicationally.\n\"Oh yes, do not long,\" said Mr. Malfoy sharply to his bedroom.  \"She's sorry that,\" said Mr. Crouch.  \"These in th\nstep 9999: train loss 0.8108, val loss 1.0620\n\t\"What do you need to do?\" said Harry, slapping his hand hoping as Riddle's, skiddening with some smile.  \"You're going back to you, Granger, though.  You've got was wrong at Fleur to the Quidditch World Cup in my Head Boy.\"\n\"What's the wrong dementor work the Quidditch cup?\" sas Ernie Maxime as the dog thought going beside Madame Maxime now as it crowded at every.  \"So I'm sure about you come out of bed reading?\"\n\"Cediwiches?\" Harry asked Fudge, his heavily mouth looking round at Ron and hearing\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Add this function to your code\ndef generate_with_seed(seed_str, max_new_tokens=500, temperature=0.5):\n    # Encode the seed string\n    seed_encoded = encode(seed_str)\n    \n    # Convert to tensor and add batch dimension\n    context = torch.tensor(seed_encoded, dtype=torch.long, device=device).unsqueeze(0)\n    \n    # Generate continuation\n    generated = m.generate(context, max_new_tokens)[0].tolist()\n    \n    # Combine seed with generated tokens\n    full_sequence = seed_encoded + generated[len(seed_encoded):]\n    \n    return decode(full_sequence)\n\n# Usage example\n\n# seed_text = \"ROMEO:\"  # Try different Shakespearean prompts\n# generated_story = generate_with_seed(seed_text, max_new_tokens=500)\n# print(generated_story)\n\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T12:22:01.821514Z","iopub.execute_input":"2025-03-03T12:22:01.821845Z","iopub.status.idle":"2025-03-03T12:22:10.575521Z","shell.execute_reply.started":"2025-03-03T12:22:01.821820Z","shell.execute_reply":"2025-03-03T12:22:10.574620Z"}},"outputs":[{"name":"stdout","text":"\t(grounds were came much, untied, many monster.\nTwo looks of guidies rampaged one robes on the cocastle.  He lived slightly to find the place to a trembled orpleasant that was with some odd, and it was my soft, Harry, and Fleur was neither Sister in brain.  Harry pulled it another, and the tremendously sat down to hear his voice.  He didn't want more loud than a voice of his wand more and number on a horrible surrounded with the rest of the game.\nIt had sleep spending subject things a difficult s\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"torch.save(model.state_dict(), 'gpt_model.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T12:17:45.531552Z","iopub.execute_input":"2025-03-03T12:17:45.531768Z","iopub.status.idle":"2025-03-03T12:17:45.675624Z","shell.execute_reply.started":"2025-03-03T12:17:45.531750Z","shell.execute_reply":"2025-03-03T12:17:45.674685Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"print(generate_with_seed(\"harry was running down the garden\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T12:22:34.845447Z","iopub.execute_input":"2025-03-03T12:22:34.845761Z","iopub.status.idle":"2025-03-03T12:22:43.544892Z","shell.execute_reply.started":"2025-03-03T12:22:34.845734Z","shell.execute_reply":"2025-03-03T12:22:43.544027Z"}},"outputs":[{"name":"stdout","text":"harry was running down the garden, across the\nPumpkin Bulgarian Hornbeak. He gulped and uncovered it had been\nmatchful never.\n\n\"There's nothing what they burnt all the Quidditch Cup, they're going to borget to\nfive these points?\"\n\n\"The left -- excusemmble --\" A common huge Quigly and his feet stormering\ndown in his hands. \"He wasn't just looking for a bravery book -- ancieve\n\nhad he's been so busy and suck -- but then he told all my barrier to\nget them... trulb, without more all my wand....\n\n\"Where is it?\" said the sweet of shi\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"model2 = GPTLanguageModel()\ncheckpoint = torch.load(\"/kaggle/working/gpt_model.pth\")\nmodel2.load_state_dict(checkpoint)\nmodel2.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T12:31:37.557246Z","iopub.execute_input":"2025-03-03T12:31:37.557550Z","iopub.status.idle":"2025-03-03T12:31:37.817347Z","shell.execute_reply.started":"2025-03-03T12:31:37.557531Z","shell.execute_reply":"2025-03-03T12:31:37.816553Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-19-fdde521132c4>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(\"/kaggle/working/gpt_model.pth\")\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"GPTLanguageModel(\n  (token_embedding_table): Embedding(91, 384)\n  (position_embedding_table): Embedding(256, 384)\n  (blocks): Sequential(\n    (0): Block(\n      (sa): MultiHeadAttention(\n        (heads): ModuleList(\n          (0-5): 6 x Head(\n            (key): Linear(in_features=384, out_features=64, bias=False)\n            (query): Linear(in_features=384, out_features=64, bias=False)\n            (value): Linear(in_features=384, out_features=64, bias=False)\n            (dropout): Dropout(p=0.2, inplace=False)\n          )\n        )\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (dropout): Dropout(p=0.2, inplace=False)\n      )\n      (ffwd): FeedFoward(\n        (net): Sequential(\n          (0): Linear(in_features=384, out_features=1536, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1536, out_features=384, bias=True)\n          (3): Dropout(p=0.2, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n    )\n    (1): Block(\n      (sa): MultiHeadAttention(\n        (heads): ModuleList(\n          (0-5): 6 x Head(\n            (key): Linear(in_features=384, out_features=64, bias=False)\n            (query): Linear(in_features=384, out_features=64, bias=False)\n            (value): Linear(in_features=384, out_features=64, bias=False)\n            (dropout): Dropout(p=0.2, inplace=False)\n          )\n        )\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (dropout): Dropout(p=0.2, inplace=False)\n      )\n      (ffwd): FeedFoward(\n        (net): Sequential(\n          (0): Linear(in_features=384, out_features=1536, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1536, out_features=384, bias=True)\n          (3): Dropout(p=0.2, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n    )\n    (2): Block(\n      (sa): MultiHeadAttention(\n        (heads): ModuleList(\n          (0-5): 6 x Head(\n            (key): Linear(in_features=384, out_features=64, bias=False)\n            (query): Linear(in_features=384, out_features=64, bias=False)\n            (value): Linear(in_features=384, out_features=64, bias=False)\n            (dropout): Dropout(p=0.2, inplace=False)\n          )\n        )\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (dropout): Dropout(p=0.2, inplace=False)\n      )\n      (ffwd): FeedFoward(\n        (net): Sequential(\n          (0): Linear(in_features=384, out_features=1536, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1536, out_features=384, bias=True)\n          (3): Dropout(p=0.2, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n    )\n    (3): Block(\n      (sa): MultiHeadAttention(\n        (heads): ModuleList(\n          (0-5): 6 x Head(\n            (key): Linear(in_features=384, out_features=64, bias=False)\n            (query): Linear(in_features=384, out_features=64, bias=False)\n            (value): Linear(in_features=384, out_features=64, bias=False)\n            (dropout): Dropout(p=0.2, inplace=False)\n          )\n        )\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (dropout): Dropout(p=0.2, inplace=False)\n      )\n      (ffwd): FeedFoward(\n        (net): Sequential(\n          (0): Linear(in_features=384, out_features=1536, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1536, out_features=384, bias=True)\n          (3): Dropout(p=0.2, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n    )\n    (4): Block(\n      (sa): MultiHeadAttention(\n        (heads): ModuleList(\n          (0-5): 6 x Head(\n            (key): Linear(in_features=384, out_features=64, bias=False)\n            (query): Linear(in_features=384, out_features=64, bias=False)\n            (value): Linear(in_features=384, out_features=64, bias=False)\n            (dropout): Dropout(p=0.2, inplace=False)\n          )\n        )\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (dropout): Dropout(p=0.2, inplace=False)\n      )\n      (ffwd): FeedFoward(\n        (net): Sequential(\n          (0): Linear(in_features=384, out_features=1536, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1536, out_features=384, bias=True)\n          (3): Dropout(p=0.2, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n    )\n    (5): Block(\n      (sa): MultiHeadAttention(\n        (heads): ModuleList(\n          (0-5): 6 x Head(\n            (key): Linear(in_features=384, out_features=64, bias=False)\n            (query): Linear(in_features=384, out_features=64, bias=False)\n            (value): Linear(in_features=384, out_features=64, bias=False)\n            (dropout): Dropout(p=0.2, inplace=False)\n          )\n        )\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (dropout): Dropout(p=0.2, inplace=False)\n      )\n      (ffwd): FeedFoward(\n        (net): Sequential(\n          (0): Linear(in_features=384, out_features=1536, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=1536, out_features=384, bias=True)\n          (3): Dropout(p=0.2, inplace=False)\n        )\n      )\n      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n  (lm_head): Linear(in_features=384, out_features=91, bias=True)\n)"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"optimizer2 = torch.optim.AdamW(model.parameters(), lr=learning_rate)\nif \"optimizer\" in checkpoint:\n    optimizer2.load_state_dict(checkpoint[\"optimizer\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T12:36:08.090319Z","iopub.execute_input":"2025-03-03T12:36:08.090580Z","iopub.status.idle":"2025-03-03T12:36:08.095183Z","shell.execute_reply.started":"2025-03-03T12:36:08.090559Z","shell.execute_reply":"2025-03-03T12:36:08.094480Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss2():\n    out = {}\n    model2.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model2.train()\n    return out\n\nm2 = model2.to(device)\nfor iter in range(20):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss2()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n        context = torch.zeros((1, 1), dtype=torch.long, device=device)\n        print(decode(m2.generate(context, max_new_tokens=500)[0].tolist()))\n        \n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model2(xb, yb)\n    optimizer2.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer2.step()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T12:36:20.806550Z","iopub.execute_input":"2025-03-03T12:36:20.806831Z","iopub.status.idle":"2025-03-03T12:37:54.154868Z","shell.execute_reply.started":"2025-03-03T12:36:20.806810Z","shell.execute_reply":"2025-03-03T12:37:54.154205Z"}},"outputs":[{"name":"stdout","text":"step 0: train loss 0.8124, val loss 1.0656\n\tE GIST EINDON REALLY HE REALIONED\nTHE DING IN TROUS ELESTILDIC AND BLIEVE HE WILL RY WEAL?  Never mind he was useful to find it against her several family day in the quidmark over the\nnearest made rippe. Drain, you were looking off.... The old Daily Potter reveal detention from Boggart\n-- and if I am sortan to mysticks in a big salesandary, elbow, and, I expelling the\npossibilisk and Mugglemies have bened, but as a margele that man, Basil,\nwho was tall...\n\n\"All right, could this moment?\" blurted\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}